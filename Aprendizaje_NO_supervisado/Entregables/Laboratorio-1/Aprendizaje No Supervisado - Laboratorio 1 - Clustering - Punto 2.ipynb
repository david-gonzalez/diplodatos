{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h4>Diplomatura AACSyA 2018 - FaMAF - UNC</h4>\n",
    "<h1>Aprendizaje No Supervisado</h1>\n",
    "<h2>Práctico 1 - Clustering</h2>\n",
    "<hr>\n",
    "Por David Gonzalez <leonardo.david.gonzalez@gmail.com> y Facundo Díaz Cobos <facundo.diaz.cobos@gmail.com>\n",
    "</center>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "En este práctico se explorarán diferentes soluciones de clustering, para desarrollar las capacidades de análisis de\n",
    "soluciones de clustering. Es preferible que los conjuntos de datos con los que trabajar sean propios, ya que de esta\n",
    "forma podrán aplicar su conocimiento del dominio en la interpretación de las diferentes soluciones. Alternativa-\n",
    "mente, pueden usar conjuntos de datos de los ejemplos de la materia.\n",
    "En los mismos, hacer una breve discusión del problema y explicar cómo puede ser útil usar técnicas de clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consignas\n",
    "Para cumplir los objetivos, realizar las siguientes actividades:\n",
    "\n",
    "1 - Explorar soluciones con diferentes parámetros y compararlas. Por ejemplo, variar el número de clusters, las\n",
    "métricas de distancia, el número de iteraciones o el número de veces que se inicializan las semillas. Describir\n",
    "brevemente: número de clusters, población de cada cluster, algunas caracterı́sticas distintivas de cada cluster,\n",
    "algunos elementos que se puedan encontrar en cada cluster.\n",
    "\n",
    "2 - Incorporar un embedding como preproceso a los datos, aplicar los algoritmos de clustering después de ese\n",
    "preproceso y describir la solución o soluciones resultantes, discutiendo las ventajas que resultan. Se pueden\n",
    "usar:\n",
    "\n",
    "◦ Principal\n",
    "Component Analysis http://scikit-learn.org/stable/modules/generated/sklearn.\n",
    "decomposition.PCA.html\n",
    "\n",
    "◦ para texto, embeddings neuronales Gensim https://pypi.org/project/gensim/\n",
    "\n",
    "◦ para texto, embeddings neuronales Fastext https://pypi.org/project/fasttext/\n",
    "\n",
    "3 - Proponer (y en lo posible, implementar) métricas de evaluación de soluciones de clustering basadas en testigos.\n",
    "Los testigos son pares de objetos que un experto de dominio etiqueta como “deberı́an estar en el mismo cluster”\n",
    "o “deberı́an estar en distintos clusters”.\n",
    "\n",
    "4 - El método k-means de scikit-learn no provee una forma sencilla de obtneer los objetos más cercanos al centroide\n",
    "de un cluster. Proponga alguna forma de obtener una muestra de los elementos de un cluster que sean cercanos\n",
    "al centroide, por ejemplo, usando clasificadores, usando distancia coseno, etc. En lo posible, implementarlos y\n",
    "mostrar esos elementos, discutir la representatividad de los elementos encontrados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>RESOLUCIÓN</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando los datos:\n",
    "Vamos a trabajar un set de datos correspondiente a compras reales de clientes realizadas en el año 2017. Los clientes fueron anonimizados previamente para poder ser utilizados en el ejercicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from yellowbrick.cluster.elbow import KElbowVisualizer\n",
    "from sklearn.cluster import KMeans\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# Configuramos el tamaño de los gráficos, en pulgadas\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 18, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Incorporar un embedding como preproceso a los datos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intentaremos hacer una reducción de dimensionalidad mediante PCA:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ppcp_log_norm = pd.read_csv('clustering-1-datos-normalizados.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "# Primero obtenemos la matriz de correlacion \n",
    "df_corr = ppcp_log_norm.corr()\n",
    "\n",
    "display(df_corr.head())\n",
    "display(df_corr.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo vemos en el gráfico:\n",
    "plt.title('Heatmap Correlation - Original',fontsize=15)\n",
    "plt.imshow(df_corr, cmap=plt.cm.get_cmap(\"YlGnBu\"), interpolation=\"nearest\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisión 1: Obtener la cantidad de features indicando el porcentaje de varianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui podemos ver que algunas features tiene mucha correlacion entre si\n",
    "# Por lo que vamos a aplicar una técnica de embedding, PCA (Principal component analisis)\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "pca = PCA(.99)\n",
    "pca.fit(ppcp_log_norm)\n",
    "df_reduced_99_pca = pca.transform(ppcp_log_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a volver a generar otro mapa de calor, par ver si la correlacion bajo y el PCA fue efectivo.\n",
    "df_red_99 =  pd.DataFrame(data=df_reduced_99_pca, columns=range(df_reduced_99_pca.shape[1]))\n",
    "#Generamos un nuevo mapa de calor para ver corroborar que la correlacion entre columnas es baja!\n",
    "df_red_99_corr = df_red_99.corr()\n",
    "# Lo vemos en el gráfico:\n",
    "plt.title('Heatmap Correlation - PCA',fontsize=15)\n",
    "plt.imshow(df_red_99_corr, cmap=plt.cm.get_cmap(\"YlGnBu\"), interpolation=\"nearest\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos la cantidad de features que mantienen el 99% de la información\n",
    "HTML(str(df_red_99.shape[1]) + ' features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisión 2: Obtener la cantidad de features analizando la relación de varianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detección de cantida de features minima que nos dé la mayor varianza\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Entrenamos con todos los features\n",
    "n_features = ppcp_log_norm.shape[0]\n",
    "\n",
    "covar_matrix = PCA(n_components = n_features) \n",
    "covar_matrix.fit(ppcp_log_norm)\n",
    "variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "\n",
    "var=np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3)*100)\n",
    "var # cumulative sum of variance explained with [n] features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos en un grafico:\n",
    "plt.ylabel('% Variance Explained')\n",
    "plt.xlabel('# of Features')\n",
    "plt.title('PCA Analysis')\n",
    "plt.plot(var)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El valor de corte está entre los 370 y 380 feature. Analizamos mas de cerca para ver el punto de corte.\n",
    "plt.ylabel('% Variance Explained')\n",
    "plt.xlabel('# of Features')\n",
    "plt.title('PCA Analysis')\n",
    "\n",
    "plt.plot(var[370:380])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La gráfica nos muestra que luego de 375 features no mejora la variance. Tomamos esa cantidad para entrenar PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui podemos ver que algunas features tiene mucha correlacion entre si\n",
    "# Por lo que vamos a aplicar una técnica de embedding, PCA (Principal component analisis)\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "pca = PCA(n_components=375)\n",
    "pca.fit(ppcp_log_norm)\n",
    "df_reduced_pca = pca.transform(ppcp_log_norm)\n",
    "# Vamos a volver a generar otro mapa de calor, par ver si la correlacion bajo y el PCA fue efectivo. 1677679\n",
    "df_red =  pd.DataFrame(data=df_reduced_pca, columns=range(df_reduced_pca.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_red.head())\n",
    "display(df_red.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos un nuevo mapa de calor para ver corroborar que la correlacion entre columnas es baja!\n",
    "# y efectivamente este metodo nos ayudo a reducir dimencionalidad sin tanta perdida de informacion\n",
    "# ya que ahora estamos en otro espacio, sin tanta perdida de informacion\n",
    "df_red_corr = df_red.corr()\n",
    "\n",
    "# Lo vemos en el gráfico:\n",
    "plt.title('Heatmap Correlation - PCA',fontsize=15)\n",
    "plt.imshow(df_red_corr, cmap=plt.cm.get_cmap(\"YlGnBu\"), interpolation=\"nearest\")\n",
    "#plt.imshow(df_corr, cmap=plt.cm.get_cmap(\"Reds\"), interpolation=\"nearest\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(str(df_red.shape[1]) + ' features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusterizamos el resultado:\n",
    "Trabajamos ahora con las columnas detectadas y reclusterizamos para ver si mejora la distribución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clusterizamos los clientes segun la similaridad de los productos que compran\n",
    "plt.clf()\n",
    "clusterClientes = KElbowVisualizer(KMeans(), k=(2,25), metric='silhouette')\n",
    "clusterClientes.fit(df_red)\n",
    "clusterClientes.poof()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, probamos con 22 clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 22\n",
    "\n",
    "# Ejecutamos la clusterización por la cantidad de clusters seleccionada:\n",
    "np.random.seed(0)\n",
    "ppcp_kmeans = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "ppcp_clusters = ppcp_kmeans.fit_predict(df_red)\n",
    "\n",
    "pd.DataFrame(ppcp_clusters).hist()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(ppcp_clusters, return_counts=True)\n",
    "\n",
    "for index in range(0, len(unique)):\n",
    "    display( '* Cantidad de clientes del CLUSTER '+str(index)+': ' + str( counts[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que los valores de distribución son parecidos al primer cluster. A continuacion probaremos poner algunas metricas de evaluacion basadas en testigos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppcp_log_norm.to_csv('clustering-1-datos-clusterizados.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
